#!/usr/bin/python

from bs4 import BeautifulSoup
import requests
import time

baseURL = 'http://localcity'

checkedURLs = []
excludedURLs = ['/github/', '/twitter/']   # These pages lead to different domains.

def checkURL(URLToCheck):
    if URLToCheck[0:4] == "http":
        return

    if URLToCheck[0:6] == "mailto":
        return

    if URLToCheck in excludedURLs:
        return

    if URLToCheck[0:1] == "#":
        return

    if not any(d['url'] == URLToCheck for d in checkedURLs):
        pageresults = {"url": "","statusCode": "","encoding": "", "title": ""}
        print('Checking:' + URLToCheck)

        fullURL = baseURL + URLToCheck
        r = requests.get(fullURL)
        soup = BeautifulSoup(r.text)

        pageresults["url"] = URLToCheck
        pageresults["statusCode"] = str(r.status_code)
        pageresults["encoding"] = r.headers.get('content-type')
        pageresults["title"] = soup.title.string
        checkedURLs.append(pageresults)

        if r.status_code == requests.codes.ok:            
            for link in soup.find_all('a'):
                url = link.get('href')
                if url != None:
                    time.sleep(1)
                    checkURL(url)

checkURL('/')

print('Checked the following URLs:')
for page in checkedURLs:
    print("}")
    print("\t" + page["url"])
    print("\t" + page["statusCode"])
    print("\t" + page["encoding"])
    print("\t" + page["title"])
    print("}")

